---
title: "Processing Urban Estates Returns"
author: "Dave Lovell"
date: "14/04/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message = FALSE, warning = FALSE}
library("here")
library("readr")
library("dplyr")
library("digest")
library("magrittr")
library("stringr")
library("carutools") #devtools::install_github("davelovellCARU/carutools")
library("purrr")
library("tibble")
library("tidyr")
```
## Intro

For the time being, we're just creating a csv that makes sense and is nice to look at - so no factors or anything.

## Look see what we have here

### Read in that Data!

```{r readData, message=FALSE}
responses <- readr::read_csv(here::here("data/urbanContextData.csv"))
spec(responses)
```

## Create Unique IDs

Use hashed phone numbers, emails and postcodes. But first check that they are unique.

```{r checkphoneNumsUnique}
responses %$%
  paste0(pers_phone,pers_email,pers_postcode) %>% 
  sapply(digest::sha1) %>% 
  str_sub(1,8) -> digested

if(!(length(digested) == length(unique(digested))))
  error("Unique IDs are not unique!") else responses$uniqueId = digested

#Bring uniqueId to the front
responses %<>% relocate(uniqueId)
```

### Squish all strings

(Remove double and trailing whitespace from character columns)

```{r stringSquish}
responses %<>%
  mutate(across(is.character,str_squish))
```

### Capitalise names

```{r namecapitalisation}
responses %<>% mutate(pers_name = str_to_title(pers_name))
```

### Check emails

Do they have an '@' sign ect.?

```{r checkEmails, collapse = TRUE}
  # Do any email addresses contain whitespace?
  any(str_detect(responses$pers_email, "[:space:]"), na.rm = TRUE)
  # Do all emails look a least vaguely like an email?
  all(str_detect(responses$pers_email, ".+@.+\\..+"), na.rm = TRUE)
```

### Check Phone Numbers

And remove all whitespace

```{r checkPhones, collapse = TRUE}
responses %<>% mutate(pers_phone = str_remove_all(pers_phone, "[:space:]"))

all(str_detect(responses$pers_phone,"\\D", negate = TRUE), na.rm = TRUE)

all(nchar(responses$pers_phone) == 11, na.rm = TRUE)

#Not all phone numbers are eleven digits long.

responses %>% 
  select(uniqueId, pers_phone) %>% 
  filter(nchar(pers_phone) != 11)  %>% 
  mutate(pers_phone = paste0(str_sub(pers_phone,1,4), "[REDACTED]"))

## That 10 digit phone number matches the one on the paper form - it's a respondent mistake
```

### Check Personal Postcodes

```{r checkPostcodes, collapse = TRUE}
sum(str_detect(responses$pers_postcode, carutools::ct_postcode_regex(), negate = TRUE), na.rm = TRUE)

# There were 5 of these the first time the above line ran! All confusions between i's, 1's, 0's and O's, now corrected in the csv. As for the above:

responses %<>% mutate(pers_postcode = str_remove(pers_postcode, "[:space:]*London$"))
```

### Check ages
```{r ageCheck, collapse = TRUE}
all(responses$pers_age > 10, na.rm = TRUE)
all(responses$pers_age < 114, na.rm = TRUE)
```

### Tidy Up Gender Data

```{r tidyGender, collapse = TRUE}
responses %<>% mutate(pers_gender = str_to_lower(pers_gender))

unique(responses$pers_gender)

responses %<>% mutate(pers_gender = 
                        pers_gender %>% 
                        {replace(.,.=="m", "male")} %>% 
                        {replace(.,.=="f", "female")})

unique(responses$pers_gender)
```

### Polish Estate Names

```{r estatePolishing}
responses %<>% mutate(con_estate = str_to_title(con_estate))
```

### Check Leadership Ratings

They all need to be between 1 and 10

```{r checkLeadNumbers, collapse = TRUE}
responses %>% 
  summarise(across(starts_with("lead_"), ~all(.<=10 & .>=1, na.rm = TRUE))) %>% 
  glimpse
```

### Extract Postcodes from Church Location

```{r postcodeExtract, collapse = TRUE}
# Which things don't have postcodes?
responses %>% 
  select(uniqueId, church_location) %>% 
  filter(negate(ct_contains_postcode)(church_location))

# Extract postcodes
responses %<>%
  mutate(church_postcode = ct_extract_postcode(church_location))

responses %<>% relocate(church_postcode, .before = church_location)
```

### Expanding Descriptive Word strings into Logical Columns

```{r expandingWordStrings}
#Make lowercase
responses %<>% mutate(across(all_of(c("grew_words", "now_words")), str_to_lower))

# Split each string at ';', then get the unique words
responses %>%
  rowwise %>% 
  summarise(across(all_of(c("grew_words", "now_words")),
            ~{
              list(str_split(.,";", simplify = TRUE) %>%
                str_trim %>% 
                unique) 
            })) %>% 
  ungroup %>% 
  summarise(across(everything(), ~ list(unique(unlist(.))))) %$% 
  unique(unlist(grew_words), unlist(now_words)) -> uniqueWords

uniqueWords

### Function to make columns from unique words

makeWordCols <- function(string = NULL, uniqueWords = NULL, sep = ";") {
  
  wordsVector = str_split(string, pattern = sep, simplify = TRUE) %>%
    str_trim
  
  anchoredWordRegexList = as.list(paste0("^", uniqueWords, "$"))
  
  wordCols <- map_lgl(anchoredWordRegexList, ~ any(str_detect(wordsVector,.)))
  names(wordCols) <- uniqueWords %>% 
    str_to_title %>% 
    str_remove_all("[:space:]") %>% 
    str_remove_all("-") %>% 
    {paste0(str_to_lower(str_sub(.,1,1)), str_sub(.,2,-1))}
  
  wordCols <- as_tibble_row(wordCols)
  return(wordCols)
}

responses <- responses %>% 
  group_by_at(vars(-now_words, -grew_words)) %>% 
  summarise(across(all_of(c("now_words", "grew_words")),
                    ~ makeWordCols(string = ., uniqueWords = uniqueWords))) %>% 
   unpack(all_of(c("now_words", "grew_words")), names_sep = "_") %>% 
  ungroup

# Relocate new columns
responses %<>% relocate(starts_with("now_words"),  .before = grew_description)
responses %<>% relocate(starts_with("grew_words"), .before = fj_faithJourney)

## Look at it all one last time...

responses %>% 
  rename_all( ~ str_sub(.,1, 21)) %>% 
  mutate(across(is.character,~ str_sub(.,1,13))) %>% 
  glimpse

write.csv(responses, here::here("data/urbanContextResponses.csv"), row.names = FALSE, quote = TRUE)
```
